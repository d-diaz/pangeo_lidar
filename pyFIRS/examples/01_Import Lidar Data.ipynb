{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from pyFIRS.wrappers import lastools\n",
    "from pyFIRS.utils import validation_summary, move_invalid_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)\n",
    "c = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enough already, let's get to work with some lidar data\n",
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the raw lidar data is currently stored\n",
    "src_dir = '/storage/lidar/olc_metro_2014/src'\n",
    "src_tiles = glob.glob(os.path.join(src_dir, '*.laz'))\n",
    "src_epsg = 4759 # geographic coordinate system, NAD83(NSRS2007) \n",
    "\n",
    "target_epsg = 26910 # utm 10 N\n",
    "\n",
    "workdir = os.path.abspath('/storage/lidar/olc_metro_2014/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data handling directories\n",
    "raw = os.path.join(workdir,'raw')\n",
    "\n",
    "num_cores = len(c.ncores()) # identify how many workers we have\n",
    "\n",
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([src_dir, raw, las, src_epsg, target_epsg, num_cores], broadcast=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory.\n",
    "\n",
    "When we define functions using the `dask.delayed` decorator, the function will have 'lazy' instead of 'eager' execution. We can map the function to a list of inputs and it will not execute for any of them until we ask for results to be computed. When we use the `compute()` method for the client managing the scheduler that sends jobs to the workers, it then starts running the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def import_tile(tile_id): # the function we'll map to a list of inputs\n",
    "    if os.path.exists(os.path.join(raw, tile_id + '.laz')):\n",
    "        pass\n",
    "    else:\n",
    "        proc_import =  las.las2las(i=os.path.join(src_dir, tile_id + '.laz'),\n",
    "                                   drop_withheld=True, # drop any points flagged as withheld by vendor\n",
    "                                   drop_class=(7,18), # drop any points classified as noise by vendor\n",
    "#                                    epsg=src_epsg, # specify the source lidar projection in case it isn't automatically recognized\n",
    "                                   target_epsg=target_epsg, # reproject to utm zone 10 N\n",
    "                                   dont_remove_empty_files=True,\n",
    "                                   odir=raw,\n",
    "                                   olaz=True) # compress .laz file output\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, validate that the data match LAS specifications and have not been corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def validate(tile_id):\n",
    "    if os.path.exists(os.path.join(raw, tile_id + '.xml')):\n",
    "        pass\n",
    "    else:\n",
    "        proc_validate = las.lasvalidate(i=os.path.join(raw, tile_id + '.laz'),\n",
    "                               o=os.path.join(raw, tile_id + '.xml'))\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which are used, for example, when retiling and adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_index(tile_id): # the function we'll map to a list of inputs\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(raw, basename)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(raw, tile_id + '.lax')): \n",
    "        proc_index = las.lasindex(i=infile)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-build the computational graph\n",
    "Define the recipe for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ids = [os.path.basename(file).split('.')[0] for file in src_tiles]\n",
    "\n",
    "get_data = {} # a dictionary that will be used to define dask's computational graph\n",
    "for tile in tile_ids:\n",
    "    get_data['import-{}'.format(tile)]=(import_tile, tile)\n",
    "    get_data['validate-{}'.format(tile)]=(validate, 'import-{}'.format(tile))\n",
    "    get_data['index-{}'.format(tile)]=(make_index, 'validate-{}'.format(tile))\n",
    "    \n",
    "# this empty function will be added to recipe for computations\n",
    "# it will be defined to depend upon all previous steps being completed\n",
    "@dask.delayed\n",
    "def done_importing(*args, **kwargs):\n",
    "    return\n",
    "\n",
    "get_data['done_importing']=(done_importing, ['index-{}'.format(tile) for tile in tile_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_graph = c.get(get_data, 'done_importing') # builds the computational graph\n",
    "get_data_results = c.persist(get_data_graph) # starts executing it\n",
    "progress(get_data_results) # progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(get_data_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2465"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASvalidate Summary\n",
      "====================\n",
      "Passed: 2,465\n",
      "Failed: 0\n",
      "Warnings: 0\n",
      "ParseErrors: 0\n",
      "\n",
      "Details\n",
      "========\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_summary(xml_dir=raw, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_invalid_tiles(xml_dir=raw, dest_dir=os.path.join(raw, 'invalid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.close()\n",
    "# cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
