{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from pyFIRS.wrappers import lastools, fusion\n",
    "from pyFIRS.utils import clean_dir, clip_tile_from_shp, convert_project, PipelineError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)\n",
    "c = Client(cluster)\n",
    "num_cores = len(c.ncores()) # identify how many workers we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the imported lidar data is currently stored\n",
    "workdir = os.path.abspath('/storage/lidar/odf_northwest_2015/clatsop/')\n",
    "\n",
    "# define data handling directories\n",
    "interim = os.path.join(workdir,'interim')\n",
    "processed = os.path.join(workdir,'processed')\n",
    "layers = os.path.join(interim, 'layers')\n",
    "\n",
    "# the coordinate reference system we'll be working with\n",
    "target_epsg = 26910 # utm 10 N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(tile_id, process, error_msg):\n",
    "    logfile = os.path.join(interim, 'failed', tile_id + '.txt')\n",
    "    os.makedirs(os.path.dirname(logfile), exist_ok=True)\n",
    "    \n",
    "    with open(logfile, '+w') as f:\n",
    "        f.write('{} | {}: {}'.format(tile_id, process, error_msg))\n",
    "    \n",
    "    return\n",
    "\n",
    "def has_error(tile_id):\n",
    "    errors = glob.glob(os.path.join(interim, 'failed', '*.txt'))\n",
    "    tiles_with_errors = [os.path.basename(error).split('.')[0] for error in errors]\n",
    "    if tile_id in tiles_with_errors:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "strata_cols_to_grid = {'Elev strata (below 0.15) return proportion':'strat0_return-proportion',\n",
    "                       'Elev strata (0.15 to 1.37) return proportion':'strat1_return-proportion',\n",
    "                       'Elev strata (5.00 to 10.00) return proportion':'strat2_return-proportion',\n",
    "                       'Elev strata (10.00 to 20.00) return proportion':'strat3_return-proportion',\n",
    "                       'Elev strata (20.00 to 30.00) return proportion':'strat4_return-proportion',\n",
    "                       'Elev strata (above 30.00) return proportion':'strat5_return-proportion',\n",
    "                       'Int strata (below 0.15) median':'strat0_intensity-median',\n",
    "                       'Int strata (0.15 to 1.37) median':'strat1_intensity-median',\n",
    "                       'Int strata (1.37 to 5.00) median':'strat2_intensity-median',\n",
    "                       'Int strata (5.00 to 10.00) median':'strat3_intensity-median',\n",
    "                       'Int strata (10.00 to 20.00) median':'strat4_intensity-median',\n",
    "                       'Int strata (above 30.00) median':'strat5_intensity-median'\n",
    "                      }\n",
    "\n",
    "elevation_cols_to_grid = {'Elev P05':'height_05-percentile',\n",
    "                          'Elev P25':'height_25-percentile',\n",
    "                          'Elev P50':'height_50-percentile',\n",
    "                          'Elev P75':'height_75-percentile',\n",
    "                          'Elev P95':'height_95_percentile',\n",
    "                          'Elev maximum':'height_max'\n",
    "                         }\n",
    "\n",
    "topo_cols_to_grid = {'Elevation':'elevation',\n",
    "                     'Slope (degrees)':'slope',\n",
    "                     'Aspect (degrees, azimuth)':'aspect',\n",
    "                     'Profile curvature * 100':'profile_curvature',\n",
    "                     'Plan curvature * 100':'plan_curvature',\n",
    "                     'Solar Radiation Index':'solar_radiation_index',\n",
    "                     'Overall Curvature':'overall_curvature'\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([interim, processed, layers, las, fus, \n",
    "           target_epsg, num_cores, has_error, log_error,\n",
    "           strata_cols_to_grid, topo_cols_to_grid, elevation_cols_to_grid], \n",
    "          broadcast=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Canopy GridMetrics\n",
    "Calculate forest attributes using the FUSION `gridmetrics` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_gridmetrics(tile_id):\n",
    "    infile = os.path.join(interim, 'classified', tile_id + '.laz')\n",
    "    groundfile = os.path.join(interim, 'dtm_ground_tiles', tile_id + '.dtm')\n",
    "    odir = os.path.join(interim, 'gridmetrics')\n",
    "    outfile = os.path.join(odir, tile_id + '.csv')\n",
    "    \n",
    "    # get the latitude of the tile centroid\n",
    "    gdf = gpd.read_file(os.path.join(interim, 'tile_boundaries', tile_id+'.shp'))\n",
    "    latlon = gdf.exterior.centroid.to_crs({'init':'EPSG:4326'})\n",
    "    latitude = latlon.geometry.y.values[0]\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = fus.gridmetrics(groundfile=groundfile,\n",
    "                                       heightbreak=1.37, # breast height, in meters\n",
    "                                       cellsize=10, # in units of lidar data\n",
    "                                       outputfile=outfile,\n",
    "                                       datafiles=infile,\n",
    "                                       strata=(0.15, 1.37, 5.0, 10.0, 20.0, 30.0),\n",
    "                                       intstrata=(0.15, 1.37, 5.0, 10.0, 20.0, 30.0),\n",
    "                                       las_class=(0,1,2,3,4,5),\n",
    "                                       topo=(10,latitude),\n",
    "                                       odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "                \n",
    "            except PipelineError as e:\n",
    "                        log_error(tile_id, 'make_gridmetrics', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "                \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2grid(tile_id, csvfile, col_num, col_name):\n",
    "    outfile = os.path.join(interim, 'gridmetrics', 'rasters', \n",
    "                           '{}_{}.asc'.format(tile_id, col_name))\n",
    "    odir = os.path.dirname(outfile)\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = fus.csv2grid(inputfile=csvfile,\n",
    "                                    column=col_num,\n",
    "                                    outputfile=outfile,\n",
    "                                    odir=odir)\n",
    "                \n",
    "            except PipelineError as e:\n",
    "                        log_error(tile_id, 'csv2grid', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "                \n",
    "    return tile_id, proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def batch_csv2grid(tile_id):\n",
    "    # read the csv containing strata data, identify the columns to extract\n",
    "    strata_data = os.path.join(interim, 'gridmetrics', tile_id + '_all_returns_strata_stats.csv')\n",
    "    with open(strata_data) as f:\n",
    "        header = f.readline()\n",
    "        cols = header.split(',')\n",
    "        strata_columns = [{'col_num': cols.index(col),\n",
    "                           'col_name': strata_cols_to_grid[col]}\n",
    "                          for col in cols if col in strata_cols_to_grid.keys()]\n",
    "\n",
    "    for col in strata_columns:\n",
    "        strata_proc = csv2grid(tile_id, strata_data, col['col_num'], col['col_name'])\n",
    "\n",
    "    \n",
    "    # read the csv containing topo data, identify the columns to extract\n",
    "    topo_data = os.path.join(interim, 'gridmetrics', tile_id + '_topo_metrics.csv')\n",
    "    with open(topo_data) as f:\n",
    "        header = f.readline()\n",
    "        cols = header.split(',')\n",
    "        topo_columns = [{'col_num': cols.index(col),\n",
    "                         'col_name': topo_cols_to_grid[col]}\n",
    "                        for col in cols if col in topo_cols_to_grid.keys()]\n",
    "\n",
    "    for col in topo_columns:\n",
    "        topo_proc = csv2grid(tile_id, topo_data, col['col_num'], col['col_name'])\n",
    "\n",
    "        \n",
    "    # read the csv containing elevation data, identify the columns to extract\n",
    "    elevation_data = os.path.join(interim, 'gridmetrics', tile_id + '_all_returns_elevation_stats.csv')    \n",
    "    with open(elevation_data) as f:\n",
    "        header = f.readline()\n",
    "        cols = header.split(',')\n",
    "        elevation_columns = [{'col_num': cols.index(col),\n",
    "                              'col_name': elevation_cols_to_grid[col]}\n",
    "                             for col in cols if col in elevation_cols_to_grid.keys()]\n",
    "\n",
    "    for col in elevation_columns:\n",
    "        elev_proc = csv2grid(tile_id, elevation_data, col['col_num'], col['col_name'])\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def batch_asc2tif(tile_id):\n",
    "    infiles = glob.glob(os.path.join(interim, 'gridmetrics', 'rasters', '{}*.asc'.format(tile_id)))\n",
    "    \n",
    "    for infile in infiles:\n",
    "        dirname, basename = os.path.split(infile)\n",
    "        outfilename = basename.split('.')[0] + '.tif'\n",
    "        outfile = os.path.join(dirname, outfilename)\n",
    "    \n",
    "        if not os.path.exists(outfile):\n",
    "            if not has_error(tile_id):\n",
    "                try:\n",
    "                    convert_project(infile, '.tif', 'EPSG:{}'.format(target_epsg))\n",
    "                except Exception as e:\n",
    "                    log_error(tile_id, 'batch_asc2tif', e.message)\n",
    "        else: # output file already exists\n",
    "            pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_grid_metrics_buffer(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "    infiles = glob.glob(os.path.join(interim, 'gridmetrics', 'rasters', '{}*.tif'.format(tile_id)))    \n",
    "    in_shp = os.path.join(interim, 'tile_boundaries', tile_id + '.shp')\n",
    "    odir = os.path.join(processed, 'rasters', 'gridmetrics')\n",
    "    \n",
    "    for infile in infiles:\n",
    "        basename = os.path.basename(infile)\n",
    "        outfile = os.path.join(odir, basename)\n",
    "    \n",
    "        if not os.path.exists(outfile):\n",
    "            if not has_error(tile_id):\n",
    "                try:\n",
    "                    clip_tile_from_shp(infile, in_shp, odir)\n",
    "\n",
    "                except Exception as e:\n",
    "                    log_error(tile_id, 'remove_grid_metrics_buffer', e.message)\n",
    "        else: # output file already exists\n",
    "            pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tile_done(tile_id, *args, **kwargs):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "    \n",
    "    return tile_id\n",
    "\n",
    "@dask.delayed\n",
    "def tiles_done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 989 tiles to process\n"
     ]
    }
   ],
   "source": [
    "tile_ids = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'retiled', '*.laz'))]\n",
    "print('Found {:,d} tiles to process'.format(len(tile_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk = {}\n",
    "for tile in tile_ids:\n",
    "    dsk['batch_csv2grid-{}'.format(tile)]=(batch_csv2grid, tile)\n",
    "    dsk['batch_asc2tif-{}'.format(tile)]=(batch_asc2tif, 'batch_csv2grid-{}'.format(tile))\n",
    "    dsk['remove_grid_metrics_buffer-{}'.format(tile)]=(remove_grid_metrics_buffer, 'batch_asc2tif-{}'.format(tile))\n",
    "    dsk['tile_done-{}'.format(tile)]=(tile_done, ['remove_grid_metrics_buffer-{}'.format(tile)])\n",
    "    \n",
    "dsk['tiles_done'] = (tiles_done, ['tile_done-{}'.format(tile) for tile in tile_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_graph = c.get(dsk, 'tiles_done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_results = c.compute(tiles_graph) # this might take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9cde6926b74f67a0aca970bdad6d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress(tiles_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Summary\n",
      "--------------------\n",
      "0 tiles failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failed = glob.glob(os.path.join(interim, 'failed', '*.txt'))\n",
    "\n",
    "summary = '''Processing Summary\n",
    "--------------------\n",
    "{:,d} tiles failed\n",
    "'''.format(len(failed))\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = c.compute([make_gridmetrics(tile_id) for tile_id in tile_ids])\n",
    "# progress(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv2grid_results = c.compute([batch_csv2grid(tile_id) for tile_id in tile_ids])\n",
    "# progress(csv2grid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asc2tif_results = c.compute([batch_asc2tif(tile_id) for tile_id in tile_ids])\n",
    "# progress(asc2tif_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_results = c.compute([remove_grid_metrics_buffer(tile_id) for tile_id in tile_ids])\n",
    "# progress(clip_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get rid of the .asc files FUSION generated\n",
    "clean_dir(os.path.join(interim, 'gridmetrics', 'rasters'), ['.asc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
