{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow follows the framework of two tutorials on lidar [pre-processing](https://rapidlasso.com/2013/10/13/tutorial-lidar-preparation/) and [information extraction](https://rapidlasso.com/2013/10/20/tutorial-derivative-production/) published by Martin Isenburg. It assumes that your lidar data is in tiles and has ground returns already classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, glob, platform, subprocess\n",
    "import geopandas as gpd, pandas as pd\n",
    "import rasterio\n",
    "from matplotlib import pyplot as plt\n",
    "from pyFIRS.utils import lastools, fusion, formatters\n",
    "import ipyparallel as ipp\n",
    "from tqdm import tqdm_notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for LAStools command line tools\n",
    "las_src = '/storage/lidar/LAStools/bin/' # wherever they live\n",
    "las = lastools.useLAStools(las_src)\n",
    "# for FUSION command line tools\n",
    "fus_src = '/storage/lidar/FUSION/' # wherever they live\n",
    "fus = fusion.useFUSION(fus_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify some key parameters for the processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the raw lidar data is currently stored\n",
    "raw_tiles = '/storage/lidar/Swinomish_Lidar_2016/source/*.laz'\n",
    "workdir = os.path.abspath('/storage/lidar/Swinomish_Lidar_2016')\n",
    "\n",
    "num_cores=32 # how many cores to use for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the format of the lidar data provided by the vendor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lasinfo (180812) report for '/storage/lidar/Swinomish_Lidar_2016/source/q48122D5415.laz'\r\n",
      "reporting all LAS header entries:\r\n",
      "  file signature:             'LASF'\r\n",
      "  file source ID:             0\r\n",
      "  global_encoding:            1\r\n",
      "  project ID GUID data 1-4:   00000000-0000-0000-6557-747300004157\r\n",
      "  version major.minor:        1.2\r\n",
      "  system identifier:          'Quantum Spatial'\r\n",
      "  generating software:        'LasMonkey 2.2.6'\r\n",
      "  file creation day/year:     88/2017\r\n",
      "  header size:                227\r\n",
      "  offset to point data:       2392\r\n",
      "  number var. length records: 3\r\n",
      "  point data format:          1\r\n",
      "  point data record length:   33\r\n",
      "  number of point records:    36190794\r\n",
      "  number of points by return: 17005960 12142658 5370083 1418786 228637\r\n",
      "  scale factor x y z:         0.01 0.01 0.01\r\n",
      "  offset x y z:               0 0 0\r\n",
      "  min x y z:                  1151361.60 1124700.02 -253.61\r\n",
      "  max x y z:                  1154513.99 1129337.94 380.54\r\n",
      "variable length header record 1 of 3:\r\n",
      "  reserved             0\r\n",
      "  user ID              'LASF_Projection'\r\n",
      "  record ID            2112\r\n",
      "  length after header  1079\r\n",
      "  description          'WKT Projection'\r\n",
      "    WKT OGC COORDINATE SYSTEM:\r\n",
      "    COMPD_CS[\"NAD83(HARN) / Washington South (ftUS) + NAVD88 height (ftUS)\",PROJCS[\"NAD83(HARN) / Washington South (ftUS)\",GEOGCS[\"NAD83(HARN)\",DATUM[\"NAD83 (High Accuracy Reference Network)\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[\"EPSG\",\"6152\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4152\"]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"standard_parallel_1\",47.33333333333334],PARAMETER[\"standard_parallel_2\",45.83333333333334],PARAMETER[\"latitude_of_origin\",45.33333333333334],PARAMETER[\"central_meridian\",-120.5],PARAMETER[\"false_easting\",1640416.667],PARAMETER[\"false_northing\",0],UNIT[\"US survey foot\",0.3048006096012192,AUTHORITY[\"EPSG\",\"9003\"]],AXIS[\"X\",EAST],AXIS[\"Y\",NORTH],AUTHORITY[\"EPSG\",\"2927\"]],VERT_CS[\"NAVD88 height (ftUS)\",VERT_DATUM[\"North American Vertical Datum 1988\",2005,AUTHORITY[\"EPSG\",\"5103\"]],UNIT[\"US survey foot\",0.3048006096012192,AUTHORITY[\"EPSG\",\"9003\"]],AXIS[\"Up\",UP],AUTHORITY[\"EPSG\",\"6360\"]]]\r\n",
      "variable length header record 2 of 3:\r\n",
      "  reserved             0\r\n",
      "  user ID              'lascompatible'\r\n",
      "  record ID            22204\r\n",
      "  length after header  156\r\n",
      "  description          'by LAStools of rapidlasso GmbH'\r\n",
      "variable length header record 3 of 3:\r\n",
      "  reserved             0\r\n",
      "  user ID              'LASF_Spec'\r\n",
      "  record ID            4\r\n",
      "  length after header  768\r\n",
      "  description          'by LAStools of rapidlasso GmbH'\r\n",
      "    Extra Byte Descriptions\r\n",
      "      data type: 4 (short), name \"LAS 1.4 scan angle\", description: \"additional attributes\", scale: 0.006, offset: 0 (not set)\r\n",
      "      data type: 1 (unsigned char), name \"LAS 1.4 extended returns\", description: \"additional attributes\", scale: 1 (not set), offset: 0 (not set)\r\n",
      "      data type: 1 (unsigned char), name \"LAS 1.4 classification\", description: \"additional attributes\", scale: 1 (not set), offset: 0 (not set)\r\n",
      "      data type: 1 (unsigned char), name \"LAS 1.4 flags and channel\", description: \"additional attributes\", scale: 1 (not set), offset: 0 (not set)\r\n",
      "LASzip compression (version 2.4r1 c2 50000): POINT10 2 GPSTIME11 2 BYTE 2\r\n",
      "reporting minimum and maximum for all LAS point record entries ...\r\n",
      "  X           115136160  115451399\r\n",
      "  Y           112470002  112933794\r\n",
      "  Z              -25361      38054\r\n",
      "  intensity         115      65535\r\n",
      "  return_number       1          7\r\n",
      "  number_of_returns   1          7\r\n",
      "  edge_of_flight_line 0          0\r\n",
      "  scan_direction_flag 0          0\r\n",
      "  classification      1          7\r\n",
      "  scan_angle_rank   -32         32\r\n",
      "  user_data           0          1\r\n",
      "  point_source_ID 10140      10144\r\n",
      "  gps_time 142356073.975475 142357327.605288\r\n",
      "number of first returns:        17005960\r\n",
      "number of intermediate returns: 7043859\r\n",
      "number of last returns:         17000747\r\n",
      "number of single returns:       4859772\r\n",
      "WARNING: for return 4 real number of points by return (1418871) is different from header entry (1418786).\r\n",
      "WARNING: there are 23021 points with return number 6\r\n",
      "WARNING: there are 1564 points with return number 7\r\n",
      "overview over number of returns of given pulse: 4859772 13544679 11856335 4761909 1028308 128771 11020\r\n",
      "histogram of classification of points:\r\n",
      "        33259991  unclassified (1)\r\n",
      "         2630049  ground (2)\r\n",
      "          300754  noise (7)\r\n",
      " +-> flagged as withheld:  1877840\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vendor_tiles = glob.glob(raw_tiles)\n",
    "las.lasinfo(i=vendor_tiles[0], \n",
    "            echo=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data handling directories\n",
    "raw, interim, processed = os.path.join(workdir,'raw'), os.path.join(workdir,'interim'), os.path.join(workdir,'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done moving tiles into working directory.\n",
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "las.las2las(i=raw_tiles,\n",
    "            odir=raw,\n",
    "            drop_withheld=True, # drop any points flagged as withheld by vendor\n",
    "            drop_class=(7,18), # drop any points classified as noise by vendor\n",
    "            olaz=True,\n",
    "            cores=num_cores)\n",
    "print('Done moving tiles into working directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which we'll use for adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done adding spatial indexes.\n",
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(raw,'*.laz')\n",
    "\n",
    "las.lasindex(i=infiles, \n",
    "             cores=num_cores)\n",
    "\n",
    "print(\"Done adding spatial indexes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll retile the data to add buffers for avoiding edge effects during processing.\n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done retiling and adding buffers. Created 679 tiles.\n",
      "CPU times: user 168 ms, sys: 52 ms, total: 220 ms\n",
      "Wall time: 6min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(raw, '*.laz')\n",
    "odir = os.path.join(interim, 'retiled_v2')\n",
    "\n",
    "las.lastile(i=infiles,\n",
    "            tile_size=1000, # in units of lidar data\n",
    "            buffer=100, # assumes units are in feet... if using meters, change to 25\n",
    "            flag_as_withheld=True,\n",
    "            olaz=True,\n",
    "            odir=odir,\n",
    "            cores=num_cores)\n",
    "\n",
    "new_tiles = glob.glob(os.path.join(odir,'*.laz'))\n",
    "print('Done retiling and adding buffers. Created {} tiles.'.format(len(new_tiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you need to save on storage space\n",
    "# clean out the raw directory now that you have retiled data to work with\n",
    "# shutil.rmtree(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify points in the lidar point cloud\n",
    "Remove noise and identify high vegetation and buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done denoising tiles.\n",
      "CPU times: user 116 ms, sys: 24 ms, total: 140 ms\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles=os.path.join(interim, 'retiled_v2', '*.laz')\n",
    "odir = os.path.join(interim, 'denoised_v2')\n",
    "\n",
    "las.lasnoise(i=infiles, \n",
    "             remove_noise=True,\n",
    "             odir=odir, \n",
    "             olaz=True, \n",
    "             cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done denoising tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the height aboveground for each point for use in classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating height above ground.\n",
      "CPU times: user 236 ms, sys: 60 ms, total: 296 ms\n",
      "Wall time: 7min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles=os.path.join(interim, 'denoised_v2', '*.laz')\n",
    "odir = os.path.join(interim, 'lasheight_v2')\n",
    "\n",
    "las.lasheight(i=infiles,\n",
    "              odir=odir, \n",
    "              olaz=True, \n",
    "              cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done calculating height above ground.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll classify points (that haven't already been classified into meaningful categories) as building or high vegetation that meet certain criteria for 'planarity' or 'ruggedness'. \n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done classifying lidar tiles.\n",
      "CPU times: user 224 ms, sys: 72 ms, total: 296 ms\n",
      "Wall time: 28min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'lasheight_v2', '*.laz')\n",
    "odir = os.path.join(interim, 'classified_v2')\n",
    "\n",
    "las.lasclassify(i=infiles,\n",
    "                odir=odir,\n",
    "                olaz=True,\n",
    "                step=5, # if your data are in meters, the LAStools default is 2.0\n",
    "                planar=0.5, # if your data are in meters, the LAStools default is 0.1\n",
    "                rugged=1, # if your data are in meters, the LAStools default is 0.4\n",
    "                ignore_class=(2,9,10,11,13,14,15,16,17), # ignore points already classified meaningfully\n",
    "                cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done classifying lidar tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remove buffer points from the classified tiles and put the clean tiles in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done trimming classified lidar tiles.\n",
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(processed, 'points')\n",
    "\n",
    "las.las2las(i=infiles,\n",
    "            odir=odir,\n",
    "            olaz=True,\n",
    "            drop_withheld=True, # remove points in tile buffers that were flagged as withhled with lastile\n",
    "            set_user_data=0, # remove height aboveground calculated using lasheight\n",
    "            cores=num_cores)\n",
    "\n",
    "print('Done trimming classified lidar tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a shapefile showing the layout of the tiles of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced a shapefile overview of clean tile boundaries.\n",
      "CPU times: user 8 ms, sys: 8 ms, total: 16 ms\n",
      "Wall time: 4.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'points', '*.laz')\n",
    "odir = os.path.join(processed, 'vectors')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                o='tiles.shp',\n",
    "                oshp=True,\n",
    "                use_bb=True, # use bounding box of tiles\n",
    "                overview=True,\n",
    "                labels=True,\n",
    "                cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Produced a shapefile overview of clean tile boundaries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# remove intermediate lidar files if you want to reclaim storage space\n",
    "# shutil.rmtree(os.path.join(interim, 'retiled'))\n",
    "# shutil.rmtree(os.path.join(interim, 'denoised'))\n",
    "# shutil.rmtree(os.path.join(interim, 'lasheight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a bare earth Digital Elevation Model\n",
    "Generate tiles of the bare earth model. This assumes that there are already ground-classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing bare earth tiles.\n",
      "CPU times: user 24 ms, sys: 8 ms, total: 32 ms\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(processed, 'rasters', 'DEM_tiles')\n",
    "\n",
    "las.las2dem(i=infiles,\n",
    "            odir=odir,\n",
    "            otif=True, # create tiles as GeoTiff rasters\n",
    "            keep_class=2, # keep ground-classified returns only\n",
    "            thin_with_grid=1, # use a 1 x 1 resolution for creating the TIN for the DEM\n",
    "            step=1, # grid cell size to use for raster, in units of lidar data\n",
    "            extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "            use_tile_bb=True, # remove buffers from tiles\n",
    "            cores=num_cores) \n",
    "\n",
    "# specify the projection for all GeoTiffs using rasterio command line tool\n",
    "for file in glob.glob(os.path.join(processed, 'rasters', 'DEM_tiles', '*.tif')):\n",
    "    subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:2286', file],\n",
    "                   stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "print('Done producing bare earth tiles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 76 files with extension .kml.\n",
      "Removed 76 files with extension .tfw.\n"
     ]
    }
   ],
   "source": [
    "dir_to_clean = os.path.join(processed,'rasters','DEM_tiles')\n",
    "clean_dir(dir_to_clean, ['.kml', '.tfw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single DEM formatted as a GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing merged DEM GeoTiff.\n",
      "CPU times: user 8 ms, sys: 692 ms, total: 700 ms\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'rasters', 'DEM_tiles', '*.tif')\n",
    "outfile = os.path.join(processed, 'rasters', 'dem.tif')\n",
    "\n",
    "# merge the GeoTiffs using rasterio command line tool\n",
    "subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "               stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "\n",
    "print('Done producing merged DEM GeoTiff.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a hillshade layer, we'll first, generate hillshade tiles from the bare earth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing hillshade bare earth tiles.\n",
      "CPU times: user 12 ms, sys: 16 ms, total: 28 ms\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(processed, 'rasters', 'hillshade_tiles')\n",
    "\n",
    "las.las2dem(i=infiles,\n",
    "            odir=odir,\n",
    "            otif=True, # create tiles as GeoTiffs\n",
    "            cores=num_cores,\n",
    "            hillshade=True,\n",
    "            keep_class=2, # keep ground-classified returns only\n",
    "            thin_with_grid=1, # use a 0.5 x 0.5 resolution for creating the TIN for the DEM\n",
    "            extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "            step=1, # grid cell size to use for raster, in units of lidar data\n",
    "            use_tile_bb=True) # remove buffers from tiles\n",
    "\n",
    "# specify the projection for all GeoTiffs using rasterio command line tool\n",
    "for file in glob.glob(os.path.join(processed, 'rasters', 'hillshade_tiles', '*.tif')):\n",
    "    subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:2286', file],\n",
    "                   stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "\n",
    "print('Done producing hillshade bare earth tiles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 76 files with extension .kml.\n",
      "Removed 76 files with extension .tfw.\n"
     ]
    }
   ],
   "source": [
    "dir_to_clean = os.path.join(processed,'rasters','hillshade_tiles')\n",
    "clean_dir(dir_to_clean, ['.kml', '.tfw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing merged hillshade GeoTiff.\n",
      "CPU times: user 12 ms, sys: 696 ms, total: 708 ms\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'rasters', 'hillshade_tiles', '*.tif')\n",
    "outfile = os.path.join(processed, 'rasters', 'hillshade.tif')\n",
    "\n",
    "# merge the GeoTiffs using rasterio command line tool\n",
    "subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "               stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "\n",
    "print('Done producing merged hillshade GeoTiff.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify building footprints\n",
    "First start by building shapefiles showing building boundaries in each buffered tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing building footprints in buffered tiles.\n",
      "CPU times: user 12 ms, sys: 16 ms, total: 28 ms\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(interim, 'building_tiles')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                keep_class=6, # use only building-classified points\n",
    "                disjoint=True, # compute separate polygons for each building\n",
    "                concavity=3, # map concave boundary if edge length >= 3ft\n",
    "                cores=num_cores)\n",
    "\n",
    "print('Done producing building footprints in buffered tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapefiles showing the bounding box of each (unbuffered) tile that we'll use to remove buildings that fall in the buffered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing boundaries of unbuffered tiles.\n",
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'points', '*.laz')\n",
    "odir = os.path.join(interim, 'tile_boundaries')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                oshp=True,\n",
    "                use_tile_bb=True,\n",
    "                cores=num_cores)\n",
    "\n",
    "print('Done producing boundaries of unbuffered tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shapefile containing polygons of the building boundaries, we'll use the `clean_tile` function to remove polygons from a tile if their centroid falls in the buffered area of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing building footprints in cleaned (unbuffered) tiles.\n",
      "CPU times: user 11.4 s, sys: 4 ms, total: 11.4 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "building_tiles = glob.glob(os.path.join(interim, 'building_tiles', '*.shp'))\n",
    "odir = os.path.join(processed, 'vectors', 'building_tiles')\n",
    "\n",
    "for poly_shp in building_tiles:\n",
    "    fname = os.path.basename(poly_shp)\n",
    "    tile_shp = os.path.join(interim, 'tile_boundaries', fname)\n",
    "    lastools.clean_buffer_polys(poly_shp, tile_shp, odir, simp_tol=3, simp_topol=True)\n",
    "\n",
    "print('Done producing building footprints in cleaned (unbuffered) tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use geopandas to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done merging tiles of building footprints into a single shapefile.\n",
      "CPU times: user 4.29 s, sys: 12 ms, total: 4.3 s\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "building_tiles = glob.glob(os.path.join(processed, 'vectors', 'building_tiles', '*.shp'))\n",
    "# create a list of geodataframes containing the tiles of building footprints\n",
    "gdflist = [gpd.read_file(tile) for tile in building_tiles]\n",
    "# merge them all together\n",
    "merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "# using pandas concat caused us to lose projection information, so let's add that back in\n",
    "merged.crs = gdflist[0].crs\n",
    "# and write the merged data to a new shapefile\n",
    "merged.to_file(os.path.join(processed,'vectors','buildings.shp'))\n",
    "\n",
    "print('Done merging tiles of building footprints into a single shapefile.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Canopy Height Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll switch to using `FUSION` command line tools to generate a Canopy Height Model (CHM). \n",
    "\n",
    "Because `FUSION` commands don't have built-in support for multi-core processing in each command line tool, to run processing in parallel, we'll launch a parallel computing cluster using `ipyparallel`. If you have `ipyparallel` installed in the computing environment that was used to launch this notebook (which may be different from the kernel you're using to execute it), you should be able to start a parallel computing cluster by switching to the \"IPython Clusters\" tab of the \"Home\" tab that was created when you called `jupyter notebook` from the console.  Note that `pyFIRS` needs to be installed in the virtual environment used to launch this notebook.\n",
    "\n",
    "Alternatively, you can also issue a call from the command line to fire up a cluster of workers. You will need to have your virtual environment activated (`activate env_name` on Windows or `source activate env_name` on other OSs, replacing `env_name` with whatever your environment's name is), and then call:\n",
    "\n",
    "```> ipcluster start -n #``` where # is the number of cores you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/ipyparallel/client/client.py:459: RuntimeWarning: \n",
      "            Controller appears to be listening on localhost, but not on this machine.\n",
      "            If this is true, you should specify Client(...,sshserver='you@Ford')\n",
      "            or instruct your controller to listen on an external IP.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "importing subprocess on engine(s)\n",
      "importing os on engine(s)\n",
      "importing lastools from pyFIRS.utils on engine(s)\n",
      "importing fusion from pyFIRS.utils on engine(s)\n",
      "importing rasterio on engine(s)\n",
      "importing geopandas on engine(s)\n"
     ]
    }
   ],
   "source": [
    "rc = ipp.Client() # a client for controlling the workers\n",
    "print(rc.ids) # how many workers?\n",
    "\n",
    "dv = rc[:] # create a direct view of the workers\n",
    "v = rc.load_balanced_view() # create a load-balanced view of the workers\n",
    "\n",
    "# import the relevant packages to all the workers\n",
    "with dv.sync_imports():\n",
    "    import subprocess\n",
    "    import os\n",
    "    from pyFIRS.utils import lastools\n",
    "    from pyFIRS.utils import fusion\n",
    "    import rasterio\n",
    "    import geopandas\n",
    "    \n",
    "# push some objects to the workers\n",
    "dv.push(dict(raw=raw, interim=interim, processed=processed, las_src=las_src, fus_src=fus_src))\n",
    "\n",
    "\n",
    "# instantiate the useLAStools class on the workers \n",
    "# so they can each execute LAStools commands\n",
    "%px las = lastools.useLAStools(las_src) \n",
    "%px fus = fusion.useFUSION(fus_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n"
     ]
    }
   ],
   "source": [
    "# push a unique identifier to each worker that we'll use to run separate instances of WINE\n",
    "prefixes = [x for x in range(num_cores)]\n",
    "dv.scatter('wine_prefix', prefixes)\n",
    "print(dv['wine_prefix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the wineprefixes in case they doesn't already exist\n",
    "%px subprocess.run(['export WINEPREFIX=~/.wine-{}'.format(wine_prefix[0])], shell=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to track progress while an asynchronous batch is running, you can execute the following function to show a progress bar including estimated time to completion. This will lock the notebook (i.e., no other notebook cells may be executed) until the batch is completed unless you interrupt the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pbar(res):\n",
    "    jobs_done = res.progress\n",
    "    with tqdm_notebook(total=len(res), initial=jobs_done, desc='Progress', unit='tile') as pbar:\n",
    "        while not res.ready():\n",
    "            new_progress = res.progress - jobs_done\n",
    "            jobs_done += new_progress\n",
    "            pbar.update(new_progress)\n",
    "            time.sleep(0.5)\n",
    "        # once jobs are completed (i.e., res.ready() returns True)\n",
    "        # update the progress bar one last time\n",
    "        pbar.update(len(res)-jobs_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be mapping a single function to a list of inputs, and only want to specify the keyword arguments once. We'll create a wrapper function which specifies the keyword arguments and takes a single input (a tile of classified lidar data) to produce a CHM.\n",
    "\n",
    "On Linux machines, we need to utilize separate instances of WINE for parallel processing using the Windows executables. By specifying `wine_prefix`, separate WINE servers will be spun up by each worker to execute the job sent to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FUSION's `canopymodel`\n",
    "We'll first normalize all the tiles to height above ground so we don't need to provide a ground DTM for FUSION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done normalizing tiles with ground and vegetation.\n",
      "CPU times: user 96 ms, sys: 40 ms, total: 136 ms\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles=os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(interim, 'normalized')\n",
    "\n",
    "las.lasheight(i=infiles,\n",
    "              odir=odir, \n",
    "              olaz=True, \n",
    "              replace_z=True,\n",
    "              drop_below=0.1,\n",
    "              cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done normalizing tiles with ground and vegetation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a function that we'll map to a list of input files and distribute to our workers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_canopymodel(infile):\n",
    "    odir = os.path.join(interim, 'chm_tiles')\n",
    "    outname = os.path.basename(infile).split('.')[0] + '.dtm'\n",
    "    outfile = os.path.join(odir, outname)\n",
    "    return fus.canopymodel(surfacefile=outfile,\n",
    "                           odir=odir,\n",
    "                           cellsize=1,\n",
    "                           xyunits='F',\n",
    "                           zunits='F',\n",
    "                           coordsys=2, # in State Plane\n",
    "                           zone=0, # not in UTM\n",
    "                           horizdatum=2, # NAD83\n",
    "                           vertdatum=2, # NAVD88\n",
    "                           datafiles=infile,\n",
    "                           median=3, # median smoothing in nxn kernel\n",
    "                           las_class=(1,2,5), # keep only ground, unclassified, and high veg points\n",
    "                           asc=True) # also output in ascii format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started batch run, processing 76 tiles.\n"
     ]
    }
   ],
   "source": [
    "tiles_to_run = glob.glob(os.path.join(interim, 'normalized', '*.laz'))\n",
    "\n",
    "# run the batch\n",
    "res = v.map_async(run_canopymodel, tiles_to_run)\n",
    "print('Started batch run, processing {} tiles.'.format(len(tiles_to_run)))\n",
    "pbar(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dir(dir_to_clean, file_extensions):\n",
    "    '''Deletes files with specified extension(s) from a directory.\n",
    "    \n",
    "    This function is intended to help cleanup outputs from command line\n",
    "    tools that we do not want to keep. Files to be deleted will be \n",
    "    identified using a wildcard with that file extension in dir_to_clean. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_to_clean: string, path\n",
    "        path to directory to delete files from\n",
    "    file_extension: string or list-like of strings\n",
    "        file extensions that will be used for identifying files to remove,\n",
    "        such as ['.tfw', '.kml'].\n",
    "    '''\n",
    "    for ext in file_extensions:\n",
    "        to_rem = glob.glob(os.path.join(dir_to_clean, '*{}'.format(ext)))\n",
    "        for file in to_rem:\n",
    "            os.remove(file)\n",
    "        print(\"Removed {:,d} files with extension {}.\".format(len(to_rem),ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_project(infile):\n",
    "    '''Converts a raster to GeoTiff and specifies a Coordinate Reference System.'''\n",
    "    indir, file = os.path.split(infile)\n",
    "    fname = file.split('.')[0]\n",
    "    outfile = os.path.join(indir, fname + '.tif')\n",
    "    proc_trans = subprocess.run(['rio', 'convert', infile, outfile], \n",
    "                                stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    proc_project = subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:2286', outfile],\n",
    "                                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    return proc_trans, proc_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started batch run, processing 76 tiles.\n"
     ]
    }
   ],
   "source": [
    "tiles_to_run = glob.glob(os.path.join(interim, 'chm_tiles_FUSION', '*.asc'))\n",
    "\n",
    "# run the batch\n",
    "res = v.map_async(convert_project, tiles_to_run, crs='EPSG:2286')\n",
    "print('Started batch run, processing {} tiles.'.format(len(tiles_to_run)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e92c78bf6084a9db25b12273e616fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "jobs_done = res.progress\n",
    "with tqdm_notebook(total=len(res), initial=jobs_done, desc='Progress', unit='tile') as pbar:\n",
    "    while not res.ready():\n",
    "        new_progress = res.progress - jobs_done\n",
    "        jobs_done += new_progress\n",
    "        pbar.update(new_progress)\n",
    "        time.sleep(0.5)\n",
    "    # once jobs are completed (i.e., res.ready() returns True)\n",
    "    # update the progress bar one last time\n",
    "    pbar.update(len(res)-jobs_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 476 ms, total: 476 ms\n",
      "Wall time: 474 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# delete the dtm files\n",
    "dtm_files = os.path.join(interim, 'chm_tiles_FUSION', '*.dtm')\n",
    "del_files = glob.glob(dtm_files)\n",
    "for file in del_files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a655f3eaf449c38a80e9b954b48cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "jobs_done = res.progress\n",
    "with tqdm_notebook(total=len(res), initial=jobs_done, desc='Progress', unit='tile') as pbar:\n",
    "    while not res.ready():\n",
    "        new_progress = res.progress - jobs_done\n",
    "        jobs_done += new_progress\n",
    "        pbar.update(new_progress)\n",
    "        time.sleep(0.5)\n",
    "    # once jobs are completed (i.e., res.ready() returns True)\n",
    "    # update the progress bar one last time\n",
    "    pbar.update(len(res)-jobs_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_tile(infile):\n",
    "    fname = os.path.basename(infile).split('.')[0]\n",
    "    tile_shp = os.path.join(interim, 'tile_boundaries', fname+'.shp')\n",
    "    gdf = geopandas.read_file(tile_shp)\n",
    "    tile_bnds = ' '.join(str(x) for x in gdf.total_bounds)\n",
    "    odir = os.path.join(processed, 'rasters', 'chm_tiles')\n",
    "    os.makedirs(odir, exist_ok=True)\n",
    "    outfile = os.path.join(odir, fname+'.tif')\n",
    "    \n",
    "    proc_clip = subprocess.run(['rio', 'clip', infile, outfile, '--bounds', tile_bnds],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    return proc_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started batch run, processing 76 tiles.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49604f61226843209efd216853659cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tiles_to_run = glob.glob(os.path.join(interim, 'chm_tiles_FUSION', '*.tif'))\n",
    "\n",
    "# run the batch\n",
    "res = v.map_async(clip_tile, tiles_to_run)\n",
    "print('Started batch run, processing {} tiles.'.format(len(tiles_to_run)))\n",
    "pbar(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 12 ms, total: 16 ms\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = glob.glob(os.path.join(processed, 'rasters', 'chm_tiles', '*.tif'))\n",
    "outfile = os.path.join(processed, 'rasters', 'chm.tif')\n",
    "subprocess.run(['rio', 'merge', *infiles, outfile, '--co', 'compress=LZW'],\n",
    "               stderr=subprocess.PIPE, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
